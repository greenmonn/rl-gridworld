{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monte_carlo import ExactMCAgent, MCAgent\n",
    "from envs.gridworld import GridworldEnv\n",
    "from utils.grid_visualization import visualize_value_function, visualize_policy\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridWorld` 초기화하기\n",
    "\n",
    "가로로 `nx` 개, 세로로 `ny` 개의 칸을 가진 `GridworldEnv`를 만듭니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 4, 4\n",
    "env = GridworldEnv([ny, nx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-carlo '에이전트' 초기화하기\n",
    "\n",
    "드디어 우리는 '에이전트'라고 부를 수 있는 것을 다루게 되었습니다. `ExacatMCAgent` 는 __\"도박의 도시 몬테카를로 (MC) 그리고 MC 정책추정\"__ 에서 배운 vanilla 버전의 Monte-carlo 정책평가를 수행합니다. vanilla MC policy evaluation은 다음과 같은 수식으로 상태 가치함수를 추산합니다.\n",
    "\n",
    "$$ V(s) \\leftarrow \\frac{G(s)}{N(s)}$$\n",
    "$G(s)$ 는 상태 $s$의 리턴 추산치의 합. $N(s)$ 는 상태 $s$의 방문 횟수.\n",
    "\n",
    "또한, 우리가 평가하려는 정책은 행동 가치함수 $Q(s,a)$ 에 대한 '$\\epsilon$-탐욕적 정책' 이라고 생각해보겠습니다. 이제 한번 파이썬 구현체를 살펴보도록 할까요?\n",
    "\n",
    "```python\n",
    "class ExactMCAgent:\n",
    "    \"\"\"\n",
    "    The exact Monte-Carlo agent.\n",
    "    This agents performs value update as follows:\n",
    "    V(s) <- s(s) / n(s)\n",
    "    Q(s,a) <- s(s,a) / n(s,a)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 gamma: float,\n",
    "                 num_states: int,\n",
    "                 num_actions: int,\n",
    "                 epsilon: float):\n",
    "        self.gamma = gamma\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self._eps = 1e-10  # for stable computation of V and Q. NOT the one for e-greedy !\n",
    "```\n",
    "\n",
    "일단 클래스의 컨스트럭터의 인자부터 살펴볼까요? \n",
    "1. `gamma` : 감가율\n",
    "2. `num_states` : 상태공간의 크기 (서로 다른 상태의 갯수)\n",
    "3. `num_actions` : 행동공간의 크기 (서로 다른 행동의 갯수)\n",
    "4. `epsilon`: $\\epsilon$-탐욕적 정책의 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent = ExactMCAgent(gamma=1.0,\n",
    "                        num_states=nx * ny,\n",
    "                        num_actions=4,\n",
    "                        epsilon=1.0) # epsilon=1.0? -> 모든 행동을 같은 확률로 하는 정책"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좀 더 직관적인 가시화를 위해서 action 인덱스를 방향으로 바꿔줍니다.\n",
    "action_mapper = {\n",
    "    0: 'UP',\n",
    "    1: 'RIGHT',\n",
    "    2: 'DOWN',\n",
    "    3: 'LEFT'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My first '에이전트-환경' interaction\n",
    "\n",
    "강화학습을 구현할 때 전형적인 형태의 `'에이전트-환경' interaction` 은 다음과 같습니다.\n",
    "\n",
    "```\n",
    "반복:\n",
    "    에피소드 시작\n",
    "    반복:\n",
    "        현재 상태 <- 환경으로 부터 현재 상태 관측\n",
    "        현재 행동 <- 에이전트의 정책함수(현재 상태)\n",
    "        다음 상태, 보상 <- 환경에 '현재 행동'을 가함\n",
    "        if 다음 상태 == 종결 상태\n",
    "            반복문 탈출\n",
    "    에이전트의 가치함수 평가 및 정책함수 개선\n",
    "```\n",
    "\n",
    "파이썬을 활용한 구현체를 확인해볼까요?\n",
    "\n",
    "1. 에피소드 (재) 시작하기 \n",
    "우리가 사용할 `GridworldEnv`에서는, `env.reset()`을 활용해서 주어진 환경을 재시작합니다.\n",
    "\n",
    "\n",
    "2. 환경에서 현재 상태 관측하기\n",
    "우리가 사용할 `GridworldEnv`에서는, `env.observe()` 를 활용해서 현재 상태를 관측합니다.\n",
    "\n",
    "\n",
    "3. 현재 상태로 부터 정책함수로 행동 결정하기\n",
    "`action = mc_agent.get_action(cur_state)` 을 활용해서 정책함수로 현재 상황에 대한 행동을 구할 수 있습니다.\n",
    "\n",
    "```python\n",
    "def get_action(self, state):\n",
    "    prob = np.random.uniform(0.0, 1.0, 1)\n",
    "    # e-greedy policy over Q\n",
    "    if prob <= self.epsilon:  # random\n",
    "        action = np.random.choice(range(self.num_actions))\n",
    "    else:  # greedy\n",
    "        action = self._policy_q[state, :].argmax()\n",
    "    return action\n",
    "```\n",
    "\n",
    "4. 현재 행동을 환경에 가하기\n",
    "`next_state, reward, done, info = env.step(action)` 을 활용해서 현재 상태에서 주어진 행동을 가한 후(!) 의 상태 `next_state` , 그에 따른 보상 `reward`, 다음 상태가 종결상태인지 여부 `done` 및 환경에 대한 정보 `info`를 확인 할 수 있습니다.\n",
    "\n",
    "### Note\n",
    "\n",
    "여러분들이 사용할 모든 환경이 `env.reset()`, `'env.step()'` 과 같이 표준화된 인터페이스를 제공하지 않을수도 있습니다. 다행히도, `gym` 환경을 상속받아 만들어진 환경들은 앞서 설명드린 표준화된 인터페이스를 갖추는 것을 권장하고 있습니다. 차후에 `gym` 의 환경을 상속 받은 환경을 사용하시게 된다면 표준화된 인터페이스가 구현되어있는 지 확인해보시는 것도 좋을것같네요. 또 여러분들께서 직접 환경을 구축하게 된다면, 해당 인터페이스를 구현하시는게 좋겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "step_counter = 0\n",
    "while True:\n",
    "    print(\"At t = {}\".format(step_counter))\n",
    "    env._render()\n",
    "    \n",
    "    cur_state = env.observe()\n",
    "    action = mc_agent.get_action(cur_state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    print(\"state : {}\".format(cur_state))\n",
    "    print(\"aciton : {}\".format(action_mapper[action]))\n",
    "    print(\"reward : {}\".format(reward))\n",
    "    print(\"next state : {} \\n\".format(next_state))\n",
    "    step_counter += 1\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-calro 정책 평가\n",
    "\n",
    "이제 Vanilla version의 Monte-carlo Policy evaluation을 수행해보도록 할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent):\n",
    "    env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    while True:\n",
    "        state = env.observe()\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode = (states, actions, rewards)\n",
    "    agent.update(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `agent.update(episode)` ?\n",
    "\n",
    "우리 실습에서는 every-visit Monte-carlo 정책 평가를 활용해서 정책에 해당하는 가치함수를 추산할 것입니다. \n",
    "\n",
    "Monte-carlo 정책평가는 하나의 온전한 에피스드가 필요했던거 다들 기억하시죠? 따라서 하나의 에피소드가 끝난 후에 `agent.update(episode)`를 수행하게 됩니다. 그러면 `agent.update(episode)` 에서는 무슨일이 일어날까요?\n",
    "\n",
    "```python\n",
    "def update(self, episode):\n",
    "    states, actions, rewards = episode\n",
    "\n",
    "    # reversing the inputs!\n",
    "    # for efficient computation of returns\n",
    "    states = reversed(states)\n",
    "    actions = reversed(actions)\n",
    "    rewards = reversed(rewards)\n",
    "\n",
    "    iter = zip(states, actions, rewards)\n",
    "    cum_r = 0\n",
    "    for s, a, r in iter:\n",
    "        cum_r *= self.gamma\n",
    "        cum_r += r\n",
    "\n",
    "        self.n_v[s] += 1\n",
    "        self.n_q[s, a] += 1\n",
    "\n",
    "        self.s_v[s] += cum_r\n",
    "        self.s_q[s, a] += cum_r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mc_agent.reset_statistics() # agent.n_v, agent.n_q, agent.s_v, agent.s_q 을 0으로 초기화 합니다.\n",
    "for _ in range(10):  \n",
    "    run_episode(env, mc_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent.compute_values()\n",
    "\n",
    "앞서 추산한 리턴의 추산치와 각 상태 $s$ 및 상태-행동 $(s,a)$ 방문 횟수를 활용해서 상태 가치함수 $V$ 와 행동 가치함수 $Q$ 를 계산합니다.\n",
    "\n",
    "```python\n",
    "def compute_values(self):\n",
    "    self.v = self.s_v / (self.n_v + self._eps)\n",
    "    self.q = self.s_q / (self.n_q + self._eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent.compute_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent.v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-carlo 방식으로 추산한 $V(s)$ 이 정말 맞을까요?\n",
    "\n",
    "우리는 이 `GridworldEnv` 에 대해서 정답을 알고 있죠? 바로 `동적 계획법`을 통해서 계산한 $V(s)$ 입니다. 여기서는 `Monte-carlo` 로 추산한 가치함수와 동적 계획법으로 계산한 가치함수의 값을 비교해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorized_dp import TensorDP\n",
    "\n",
    "dp_agent = TensorDP()\n",
    "dp_agent.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v_pi = dp_agent.policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "visualize_value_function(ax[0], mc_agent.v, nx, ny)\n",
    "_ = ax[0].set_title(\"Monte-carlo Policy evaluation\")\n",
    "\n",
    "visualize_value_function(ax[1], v_pi, nx, ny)\n",
    "_ = ax[1].set_title(\"Dynamic programming Policy evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-carlo 기법에 실망하셨나요? \n",
    "\n",
    "`dp_agent` 와 `mc_agent`에게 비슷한 시간을 주고 가치 함수를 평가해봤었는데\n",
    "`mc_agent` 의 결과가 영 시원치 않죠? 바로 `MDP` 환경모델을 활용 여부에 따른 차이입니다.\n",
    "\n",
    "`dp_agent`는 환경에 대해 훤히 알고 있으니, 짧은 시간 (혹은 계산) 만에 원하는 답을 알아내는 것은\n",
    "어쩌면 당연하겠죠. `mc_agent`에게 조금 더 시간을 줘 보는게 어떨까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eps = 2000\n",
    "log_every = 500\n",
    "\n",
    "def run_episodes(env, agent, total_eps, log_every):\n",
    "    mc_values = []\n",
    "    log_iters = []\n",
    "\n",
    "    agent.reset_statistics()\n",
    "    for i in range(total_eps+1):  \n",
    "        run_episode(env, agent)\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            agent.compute_values()\n",
    "            mc_values.append(agent.v.copy())\n",
    "            log_iters.append(i)\n",
    "    \n",
    "    info = dict()\n",
    "    info['values'] = mc_values\n",
    "    info['iters'] = log_iters\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = run_episodes(env, mc_agent, total_eps, log_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_iters = info['iters']\n",
    "mc_values = info['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_rows = len(log_iters)\n",
    "figsize_multiplier = 10\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, 2, figsize=(n_rows*figsize_multiplier*0.5, \n",
    "                                           3*figsize_multiplier))\n",
    "\n",
    "for viz_i, i in enumerate(log_iters):\n",
    "    visualize_value_function(ax[viz_i, 0], mc_values[viz_i], nx, ny,\n",
    "                            plot_cbar=False)\n",
    "    _ = ax[viz_i, 0].set_title(\"MC-PE after {} episodes\".format(i), size=20)\n",
    "\n",
    "    visualize_value_function(ax[viz_i, 1], v_pi, nx, ny,\n",
    "                             plot_cbar=False)\n",
    "    _ = ax[viz_i, 1].set_title(\"DP-PE\", size=20)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 혹시 눈치 채셨나요? 매 실행마다 결과값이 달라진다는 것을?\n",
    "\n",
    "Monte-carlo Policy evaluation 에서는 매 실행마다, 가치함수 추산값이 달라지는것을 확인하셨나요?\n",
    "그러면 한번 매 실행마다 얼마나 결과값이 다른지, 즉, 가치함수 `추산치의 분산`이 얼마나 되는지 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 10\n",
    "values_over_runs = []\n",
    "total_eps = 3000\n",
    "log_every = 30\n",
    "\n",
    "for i in range(reps):\n",
    "    print(\"start to run {} th experiment ... \".format(i))\n",
    "    info = run_episodes(env, mc_agent, total_eps, log_every)\n",
    "    values_over_runs.append(info['values'])\n",
    "    \n",
    "values_over_runs = np.stack(values_over_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_over_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pi_expanded = np.expand_dims(v_pi, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pi_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(values_over_runs - v_pi_expanded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.linalg.norm(values_over_runs - v_pi_expanded, axis=-1)\n",
    "error_mean = np.mean(errors, axis=0)\n",
    "error_std = np.std(errors, axis=0)\n",
    "\n",
    "np.save('mc_errors.npy', errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.grid()\n",
    "ax.fill_between(x=info['iters'],\n",
    "                y1=error_mean + error_std,\n",
    "                y2=error_mean - error_std,\n",
    "                alpha=0.3)\n",
    "ax.plot(info['iters'], error_mean, label='Evaluation error')\n",
    "ax.legend()\n",
    "_ = ax.set_xlabel('episodes')\n",
    "_ = ax.set_ylabel('Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Monte-carlo 정책 평가\n",
    "\n",
    "앞선 예제들에서는 Vanilla version의 MC 정책 평가에 대해서 살펴보았습니다. 이 방식은 가치함수 추산에 리턴들의 합과 각 상상태 $s$ 및 상태-행동 $(s,a)$ 방문 횟수를 따로 기록하여 두 통계치를 활용해서 가치함수들을 추산하였습니다. 이번에는 `적당히 작은` 학습률 $\\alpha$ (learning rate; lr)을 활용하는 방식을 이용해서 정책 평가를 수행해보도록 할까요?\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha (G_t - V(s))$$\n",
    "\n",
    "`MCAgent` 는 기존의 `ExacatMCAgent`와 유사하나 추가적으로 학습률 $\\alpha$  인자를 하나 더 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent = MCAgent(gamma=1.0,\n",
    "                   lr=1e-3,\n",
    "                   num_states=nx * ny,\n",
    "                   num_actions=4,\n",
    "                   epsilon=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MCAgent.update()`\n",
    "\n",
    "새로운 `MCAgent`의 학습방식이 학습률 $\\alpha$을 활용하니, 그에 따라 `update()` 함수도 조금 수정이 필요하겠죠? 수정된 `update()`함수를 살펴볼까요?\n",
    "\n",
    "```python\n",
    "def update(self, episode):\n",
    "    states, actions, rewards = episode\n",
    "\n",
    "    # reversing the inputs!\n",
    "    # for efficient computation of returns\n",
    "    states = reversed(states)\n",
    "    actions = reversed(actions)\n",
    "    rewards = reversed(rewards)\n",
    "\n",
    "    iter = zip(states, actions, rewards)\n",
    "    cum_r = 0\n",
    "    for s, a, r in iter:\n",
    "        cum_r *= self.gamma\n",
    "        cum_r += r\n",
    "\n",
    "        self.v[s] += self.lr * (cum_r - self.v[s])\n",
    "        self.q[s, a] += self.lr * (cum_r - self.q[s, a])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "    run_episode(env, mc_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "visualize_value_function(ax[0], mc_agent.v, nx, ny)\n",
    "_ = ax[0].set_title(\"Monte-carlo Policy evaluation\")\n",
    "\n",
    "visualize_value_function(ax[1], v_pi, nx, ny)\n",
    "_ = ax[1].set_title(\"Dynamic programming Policy evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCAgent, 다른 학습률 $\\alpha$에 대해선 어떨까?\n",
    "\n",
    "MCAgent의 학습률 $\\alpha$ 는 분명히 상태 (행동) 가치함수 추산에 영향을 미칠텐데, 어떻게 영향을 미치는지 알아보도록 할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc_agent_with_lr(agent, env, lr):\n",
    "    agent.reset()\n",
    "    agent.lr = lr\n",
    "    \n",
    "    for _ in range(5000):\n",
    "        run_episode(env, agent)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "    visualize_value_function(ax[0], mc_agent.v, nx, ny)\n",
    "    _ = ax[0].set_title(\"Monte-carlo Policy evaluation\")\n",
    "\n",
    "    visualize_value_function(ax[1], v_pi, nx, ny)\n",
    "    _ = ax[1].set_title(\"Dynamic programming Policy evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mc_agent_with_lr(agent=mc_agent,\n",
    "                     env=env,\n",
    "                     lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mc_agent_with_lr(agent=mc_agent,\n",
    "                     env=env,\n",
    "                     lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mc_agent_with_lr(agent=mc_agent,\n",
    "                     env=env,\n",
    "                     lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
